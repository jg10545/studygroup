# Rollin' your own neural network

Back in my day we didn't have any fancy tensor flows or pytorches. We had to compute our gradients uphill, through the snow, both ways.

## The plan

This notebook contains a derivation of all the gradients for a two-layer perceptron with sigmoid activation functions. At each step, I've added some skeleton code for you to actually implement the math, as well as a test you can use to verify your implementation.

Feel free to add whatever extra cells you need to experiment with your code.

At the end, take your new homemade AI out for a spin!





## Backpropagation

We have a cross-entropy loss function- for a single training example,

$L = -y\log\hat{y} - (1-y)\log(1-\hat{y})$

Where $y$ is the label and $\hat{y}$ is the network's output:

$\hat{y} = \sigma(s)$

* Our activation function is the sigmoid $\sigma(x) = \left(1+ e^{-x} \right)^{-1}$ 
* The input to the activation function is a linear model $s = \sum_{j}h_{j}w_{j}$
* The inputs to the linear model are weights $w_{j}$ and outputs of the hidden neurons $h_{j}$

For each hidden neuron,

$h_{j} = \sigma(s^{h}_{j})$

* The input to the activation function is $s^{h}_{j} = \sum_{k}x_{k}w^{h}_{kj}$
* The $w^{h}_{kj}$ are hidden-layer weights and $x_{k}$ are the data inputs to the model.

#### Efficiently computing for a batch

Let's say we start with a (N,K) data matrix $X$ containing $N$ length-$K$ feature vectors. To include a bias term, we'll concatenate a 1 to the end of every vector, so $X'$ will be (N,K+1). For a hidden-layer (H,K+1) weight matrix $W_{h}$:

$h = \sigma(W_{h}X^{'T})$

where $h$ is (H,N).

To add a bias term to the output layer, we can concatenate a 1 on the end of each hidden-layer output, so $h'$ will be (H+1,N). For a (H+1,1) output-layer weight matrix $w$,

$\hat{y} = \sigma(w^{T}h)^{T}$

So the output will be (N,1).









In order to do gradient descent, we need to know the partial derivative $\partial L/\partial w$ with respect to every weight in the model.






### Output layer

This layer is easier so we'll do it first. We could expand the partial derivative $\partial L/\partial w_{j}$,

$\frac{\partial L}{\partial w_{j}} = \left(\frac{\partial L}{\partial \hat{y}} \right)  \left(\frac{\partial \hat{y}}{\partial s} \right) \left(\frac{\partial s}{\partial w_{j}} \right)$

The first term in the product is computable from the first equation above:

$\frac{\partial L}{\partial \hat{y}} = \frac{-y}{\hat{y}} + \frac{1-y}{1-\hat{y}} = \frac{\hat{y}-y}{\hat{y}(1-\hat{y})}$

The second term is just the derivative of the sigmoid function:

$\frac{\partial \hat{y}}{\partial s} = \frac{\partial}{\partial s}\sigma(s) = \sigma(s)(1-\sigma(s)) = \hat{y}(1-\hat{y})$

The third is the derivative of a linear function:

$\frac{\partial s}{\partial w_{j}} = \frac{\partial }{\partial w_{j}}\sum_{j'}h_{j'}w_{j'} = h_{j}$

Putting it all together:

$\frac{\partial L}{\partial w_{j}} = \frac{\hat{y}-y}{\hat{y}(1-\hat{y})}\hat{y}(1-\hat{y})h_{j} = (\hat{y}-y)h_{j}$

So the gradient of the loss function with respect to the $j$th weight in the output layer (the weight that tells us how much to weigh the $j$th neuron in the hidden layer) is proportional to the residual of the network and the output of the $j$th neuron.

#### Efficiently computing over a batch

We want to average the gradient over a batch:

$\frac{\partial L}{\partial \vec{w}} = \frac{1}{N}\sum_{n=1}^{N}(\hat{y}_{n}-y_{n})h_{n}$

$y$ and $\hat{y}$ are (N,1), and $h'$ is (H+1,N). We can compute this part of the gradient with a single matrix multiplication:

$\frac{\partial L}{\partial \vec{w}} = \frac{1}{N}h'(\hat{y}-y)$

In my code, I didn't bother with the $1/N$ part of the gradient because we'll arbitrarily scale the learning rate anyway.






### Hidden layer

Same basic idea, but the result will be a bit more complicated.


$\frac{\partial L}{\partial w^{h}_{jk}} = \left(\frac{\partial L}{\partial s }\right) \left(\frac{\partial s}{\partial h_{j}} \right) \left(\frac{\partial h_{j}}{\partial s^{h}_{j}} \right) \left(\frac{\partial s^{h}_{j}}{\partial w^{h}_{jk}} \right)$

The first term we can pull from the output-layer step:

$\frac{\partial L}{\partial s } = \hat{y}-y$

The second term is the derivative of the output-layer linear function with respect to one of its inputs:

$\frac{\partial s}{\partial h_{j}} = w_{j}$

The third term is just the derivative of a sigmoid:

$\frac{\partial h_{j}}{\partial s^{h}_{j}} = h_{j}(1-h_{j})$

The final term is the derivative of the linear model in the $j$th hidden layer by its $k$th weight:

$\frac{\partial s^{h}_{j}}{\partial w^{h}_{jk}} = x_{k}$

Putting it all together:

$\frac{\partial L}{\partial w^{h}_{jk}} = (\hat{y}-y)w_{j}h_{j}(1-h_{j})x_{k}$






#### Efficiently computing over a batch


Just like for the output layer, we want to average the gradient over a batch:

$\frac{\partial L}{\partial w^{h}_{jk}} = \frac{1}{N}\sum_{n=1}^{N}(\hat{y}_{n}-y_{n})w_{j}h_{j,n}(1-h_{j,n})x_{k,n}$

which we could simplify to

$\frac{1}{N} w^{[K+1]}\otimes\left(\left[ h \otimes (1-h)\right] \text{diag}(\hat{y}-y)X'\right)$

where:

* $\otimes$ is the Hadamard (elementwise) product
* $\text{diag}(\vec{a})$ is a square matrix with $\vec{a}$ on the diagonal and 0 elsewhere
* $w^{[K+1]}$ a (H,K+1) array where every column is the first H elements of $w$

checking dimensions,

* $X'$ is (N,K+1)
* $h$ is (H,N)
* $h\otimes (1-h)$ is (H,N)
* $\text{diag}(\hat{y}-y)$ is (N,N)
* so $\left[ h \otimes (1-h)\right] \text{diag}(\hat{y}-y)X$ is (H,N) times (N,N) times (N,K+1) = (H,K+1)
* $w^{[K+1]}\otimes\left(\left[ h \otimes (1-h)\right] \text{diag}(\hat{y}-y)X'\right)$ is (H,K+1). 

so we'll get the same dimensions as our hidden-layer weight matrix $W_{h}$.





